{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angepasster Code zur Verarbeitung der ISCRIC Daten zu den CAMELS-DE Daten über Bodenzusammensetzung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Download der IRIC Daten -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from owslib.wcs import WebCoverageService\n",
    "from catchments import get_catchment_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Festlegung der Gebiets-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "\n",
    "output_dir = \".../input_data/isric\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download der ISRIC-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_gdf = get_catchment_gdf(ID)\n",
    "\n",
    "# Transform the catchment to the ISRIC projection\n",
    "crs_isric = 'PROJCS[\"Interrupted_Goode_Homolosine\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Interrupted_Goode_Homolosine\"],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n",
    "catchment_gdf = catchment_gdf.to_crs(crs_isric)\n",
    "\n",
    "# Download the ISRIC data\n",
    "variables = [\"sand\", \"silt\", \"clay\"]\n",
    "min_x, min_y, max_x, max_y = catchment_gdf.total_bounds\n",
    "buffer_width, buffer_height = (max_x - min_x) * 0.1, (max_y - min_y) * 0.1\n",
    "min_x, min_y, max_x, max_y = min_x - buffer_width, min_y - buffer_height, max_x + buffer_width, max_y + buffer_height\n",
    "subsets = [('X', float(min_x), float(max_x)), ('Y', float(min_y), float(max_y))]\n",
    "\n",
    "\n",
    "for variable in variables:\n",
    "    depths = [\"0-5cm\", \"5-15cm\", \"15-30cm\"]\n",
    "    depths_no_download = []\n",
    "\n",
    "    for depth in depths:\n",
    "        path = os.path.join(output_dir, variable, f\"{variable}_{depth}_mean.tiff\")\n",
    "        if os.path.exists(path):\n",
    "            with rasterio.open(path) as isric:\n",
    "                if min_x >= isric.bounds.left and max_x <= isric.bounds.right and min_y >= isric.bounds.bottom and max_y <= isric.bounds.top:\n",
    "                    depths_no_download.append(depth)\n",
    "                else:\n",
    "                    isric.close()\n",
    "                    os.remove(path)\n",
    "                    print(f\"Removed existing file {path} as it does not cover the input catchments.\")\n",
    "            isric.close()\n",
    "\n",
    "    depths = [depth for depth in depths if depth not in depths_no_download]\n",
    "\n",
    "    if depths_no_download:\n",
    "        print(f\"{variable} --- Data already exists and covers the input catchments, skipping download of {[f'{variable}_{depth}.tiff' for depth in depths_no_download]}.\")\n",
    "\n",
    "    if not depths:\n",
    "        continue\n",
    "\n",
    "    wcs = WebCoverageService(f\"http://maps.isric.org/mapserv?map=/map/{variable}.map\", version=\"2.0.1\")\n",
    "    coverage_ids = [content for content in wcs.contents if variable in content and \"mean\" in content]\n",
    "    coverage_ids = [coverage_id for coverage_id in coverage_ids if any(depth in coverage_id for depth in depths)]\n",
    "\n",
    "    variable_dir = os.path.join(output_dir, variable)\n",
    "    if not os.path.exists(variable_dir):\n",
    "        os.makedirs(variable_dir)\n",
    "\n",
    "    for coverage_id in coverage_ids:            \n",
    "        try:\n",
    "            response = wcs.getCoverage(\n",
    "                identifier=[coverage_id], \n",
    "                crs=\"EPSG:4326\",  \n",
    "                subsets=subsets, \n",
    "                resx=250, resy=250, \n",
    "                format=\"image/tiff\"\n",
    "            )\n",
    "            with open(os.path.join(variable_dir, f\"{coverage_id}.tiff\"), \"wb\") as file:\n",
    "                file.write(response.read())\n",
    "            print(f\"Downloaded {coverage_id}.tiff\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {coverage_id}: {e}\")\n",
    "\n",
    "os.system(f\"chmod -R 777 {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachverarbeitung der Daten\n",
    "(extract_iscric.R wurde in Python Skript umgewandelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the catchments\n",
    "catchments = catchment_gdf\n",
    "\n",
    "# List of variables\n",
    "variables = [\"sand\", \"silt\", \"clay\"]\n",
    "\n",
    "# Choose the depths\n",
    "depths = [\"0-5cm\", \"5-15cm\", \"15-30cm\"]\n",
    "\n",
    "# Loop over the variables\n",
    "for variable in variables:\n",
    "    print(f\"Start processing variable {variable}...\")\n",
    "\n",
    "    # Loop over the depths \n",
    "    for depth in depths:\n",
    "        filename = f\".../input_data/isric/{variable}/{variable}_{depth}_mean.tiff\"\n",
    "        with rasterio.open(filename) as src:\n",
    "            isric = src.read(1)\n",
    "            affine = src.transform\n",
    "            crs = src.crs\n",
    "\n",
    "        catchments = catchments.to_crs(crs)\n",
    "\n",
    "        # Statistics\n",
    "        stats = [\"mean\", \"min\", \"max\"]\n",
    "\n",
    "        # Extracting the raster data for all catchments\n",
    "        print(f\"Extraktion der Rasterdaten für alle Catchments für Variable {variable} und Tiefe {depth}...\")\n",
    "        extracted_rast = zonal_stats(catchments, isric, stats=stats, affine=affine, prefix=\"stat_\")\n",
    "        extracted_df = gpd.GeoDataFrame(extracted_rast)\n",
    "\n",
    "        print(extracted_df)\n",
    "\n",
    "        # Create a directory for the extracted data in case it does not exist\n",
    "        os.makedirs(f\".../input_data/isric_extracted/{variable}\", exist_ok=True)\n",
    "\n",
    "        # Save the extracted data\n",
    "        output_file = f\".../input_data/isric_extracted/{variable}/isric_{variable}_{depth}_extracted.csv\"\n",
    "        print(f\"Speichern der extrahierten Daten in {output_file}...\")\n",
    "        extracted_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten sind aus den .tiffs in .csvs extrahiert und werden nun weiterverarbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "Postprocess extracted ISRIC data.  \n",
    "- CAMELS-DE only uses the mean values in each depth.\n",
    "- Aggregate and calculate a weighted average over depths:\n",
    "    - 0-30 cm: 0-5 cm (5/30), 5-15 cm (10/30), 15-30 cm (15/30)\n",
    "- Convert to common units and rename the columns:\n",
    "    | **Variable** | **Mapped unit** | **Conversation factor** | **Common unit**   | **CAMELS-DE variable name** |\n",
    "    |--------------|-----------------|-------------------------|-------------------|-----------------------------|\n",
    "    | clay         | g/kg            | 10                      | g/100g (%)        | clay                        |\n",
    "    | silt         | g/kg            | 10                      | g/100g (%)        | silt                        |\n",
    "    | sand         | g/kg            | 10                      | g/100g (%)        | sand                        |\n",
    "\"\"\"\n",
    "\n",
    "df_result = pd.DataFrame()\n",
    "\n",
    "# Load the data\n",
    "for variable in [\"clay\", \"silt\", \"sand\"]:\n",
    "    # Get the files\n",
    "    files = glob(f\".../input_data/isric_extracted/{variable}/*.csv\")\n",
    "\n",
    "    # Create dictionary over depths with the data\n",
    "    data = {os.path.basename(file).split(f\"{variable}_\")[1].split(\"_\")[0]: pd.read_csv(file) for file in files}\n",
    "\n",
    "    # Aggregate the data\n",
    "    aggregated_data = {}\n",
    "    # 0-30 cm\n",
    "    aggregated_data[\"0-30cm\"] = (data[\"0-5cm\"].iloc[:,1:] * (5 / 30) + data[\"5-15cm\"].iloc[:,1:] * (10 / 30) + data[\"15-30cm\"].iloc[:,1:] * (15 / 30))\n",
    "\n",
    "    # Convert to common units\n",
    "    aggregated_data = {depth: df / 10 for depth, df in aggregated_data.items()}\n",
    "\n",
    "    dfs = {}\n",
    "    for depth, df in aggregated_data.items():\n",
    "        # Add the depth to the column names\n",
    "        df.columns = [f\"{depth.replace('-', '_')}_{column}\" if column != \"gauge_id\" else column for column in df.columns]\n",
    "        # Save the dataframe\n",
    "        dfs[depth] = df\n",
    "\n",
    "    # Add the camels variable names to the column names\n",
    "    for depth, df in dfs.items():\n",
    "        if variable in [\"clay\", \"silt\", \"sand\"]:\n",
    "            df.columns = [f\"{variable}_{column}\" if column != \"gauge_id\" else column for column in df.columns]\n",
    "\n",
    "    # Concatenate the dataframes, keep only the first gauge_id column\n",
    "    df_result_variable = pd.concat(dfs.values(), axis=1)\n",
    "\n",
    "    # Add the data to the result dataframe\n",
    "    df_result = pd.concat([df_result, df_result_variable], axis=1)\n",
    "\n",
    "\n",
    "# only keep columns that include _mean and the gauge_id\n",
    "df_result = df_result[[col for col in df_result.columns if \"mean\" in col or col == \"gauge_id\"]]\n",
    "\n",
    "# Remove '_stat' from column names \n",
    "df_result.columns = [col.replace('_stat', '') for col in df_result.columns]\n",
    "\n",
    "# round to 2 decimals\n",
    "df_result = df_result.round(2)\n",
    "\n",
    "df_result.insert(0, \"gauge_id\", ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abspeichern der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "file_path = f\".../output_data/camels_de/CAMELS_DE_soil_attributes.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    df_result.to_csv(file_path, index=False)  # write header if file does not exist\n",
    "else:\n",
    "    df_result.to_csv(file_path, mode='a', header=False, index=False)  # append data without writing the header\n",
    "\n",
    "print(f\"Postprocessing of all variables to soil_attributes.csv finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
